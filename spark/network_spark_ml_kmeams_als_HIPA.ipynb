{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook performs the following tasks:\n",
    "1) Read a few millions of network metrics collected from many network endpoints.\n",
    "2) Use SPARK ML K-Mean algorithm to cluster selected columns of the metrics into \n",
    "   K groups. (K = 99,999) Each group will be viewed as a \"network performance\n",
    "   profile\". \n",
    "3) Train an ALS model as a network recommender system through 'model selection'. \n",
    "   The trained model will be used to recommend better network configurations for\n",
    "   network endpoints. \n",
    "   \n",
    "   Two model selection methods are available:\n",
    "   A) SPARK CrossValidator class with a 2D parameter grid \n",
    "      (https://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "   B) my Heuristic Interleaved Parameter Alternator (HIPA)\n",
    "   \n",
    "   During tests, HIPA can save 80-90% iterations and achieve similar accuracy.\n",
    "   That is x5-x10 faster than using SPARK CrossValidator.\n",
    "\n",
    "4) Evaluate the selected (best) model based on RMSE or MAE.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "spark_sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 4 ms, total: 36 ms\n",
      "Wall time: 7.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 11.6 s (etl2M.csv has 2,374,992 rows)\n",
    "\n",
    "''' \n",
    "read data\n",
    "'''\n",
    "df_orig = spark_sql.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='false', inferschema='true')\\\n",
    "    .load('../data/etl2M.csv' if os.path.isfile('../data/etl2M.csv') else '../data/etl2M.csv.*')\n",
    "\n",
    "'''\n",
    "Use VectorAssembler to assembly interesting columns as the \"features\" for K-Mean clustering.\n",
    "ALS algorithm also needs this \"features\" column to act as the \"user characteristic\". \n",
    "The column actually contains \"latent factors\" of a network \"performance profile\" which contain\n",
    "not only status of network endpoints at a minute but also implies possible 'reconfiguration'\n",
    "or 'actions' that can be applied by an endpoint in order to achieve the performance. \n",
    "'''\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"_c1\",\"_c3\", \"_c6\", \"_c9\", \"_c12\", \"_c15\", \"_c18\"],\n",
    "    outputCol=\"features\")\n",
    "df_features = assembler.transform(df_orig)\n",
    "\n",
    "# this dataframe will be fit to K-Mean below\n",
    "df_features.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Train a K-Means model using df_feature, if not trained yet.\n",
    "!! SPARK KMeans is much slower than Intel DAAL's Kmeans...\n",
    "'''\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "'''\n",
    "How to pick a good 'K' value? We can use 'model selection' to find\n",
    "better K values, like how we'll do for ALS model selection. However,\n",
    "the ranges of possible (good) K values can be vast while Kmean model\n",
    "training on each of the possible K values can take quite some hours. \n",
    "That would total up to weeks or even months to learn better K values.\n",
    "\n",
    "Instead, here we pick a K value based on our possible ranges of each\n",
    "input column of VectorAssembler in previous cell. For example, assume\n",
    "valid _c1 values are in the range of [-90, -20], we can partition the \n",
    "range to 20 segments, each of which may exhibit similar impact on \n",
    "network performance. With similar partitioning being applied to the \n",
    "other input columns, we can multiply the number of segments of\n",
    "each partition column and it may come out with possible good K values\n",
    "in the range from hundreds of thousands to millions, each of which can\n",
    "have SPARK KMean spin its head for many many hours. So, for sake of \n",
    "energy conservation with reasonable accurcy, let's test the water in \n",
    "the lower order. Of course, we won't know how accurate the '100K' clusters\n",
    "will be until we apply them to our ALS Recommender model and data set.\n",
    "'''\n",
    "K=99999\n",
    "\n",
    "# train Kmean only if not yet, because it'll take many hours.\n",
    "path_to_save_model = \"/data/tmp/spark/KMeansModel-ml\"\n",
    "if os.path.isdir(path_to_save_model):\n",
    "    model = KMeansModel.load(path_to_save_model)\n",
    "else:\n",
    "    kmeans = KMeans(initMode=\"random\").setK(K).setSeed(1)\n",
    "    model = kmeans.fit(df_features)\n",
    "    shutil.rmtree(path_to_save_model, ignore_errors=True) \n",
    "    model.save(path_to_save_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+---+----+---+---+----+---+----+----+----+----+----+----+----+----+----+----+--------------------+----------+\n",
      "| _c0|_c1|  _c2|_c3|_c4| _c5|_c6|_c7| _c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|            features|prediction|\n",
      "+----+---+-----+---+---+----+---+---+----+---+----+----+----+----+----+----+----+----+----+----+--------------------+----------+\n",
      "|5358|  1|-40.0|-40|  0|44.0| 44| -1|46.0| 47|   1| 0.0|   0|  -1| 1.0|   1|   0|94.0|  88| -12|[1.0,-40.0,44.0,4...|      2559|\n",
      "|5294|  1|-59.0|-59| 12|94.0| 97|  2| 1.0|  1|   0| 1.0|   1|   0| 2.0|   3|   0|65.0|  90|  50|[1.0,-59.0,97.0,1...|       663|\n",
      "|5295|  0|-50.0|-47|  6|74.0| 73|  0| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|83.0| 100|  46|[0.0,-47.0,73.0,0...|      7146|\n",
      "|5290|  0|-26.0|-24|  1|74.0| 77|  0| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|55.0|  50|  25|[0.0,-24.0,77.0,0...|      4408|\n",
      "|5290|  0|-26.0|-26| -2|74.0| 67|-10| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|55.0|  25| -25|[0.0,-26.0,67.0,0...|      6041|\n",
      "|5290|  0|-26.0|-29| -3|74.0| 79| 12| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|55.0|  75|  50|[0.0,-29.0,79.0,0...|      5551|\n",
      "|5290|  0|-26.0|-27| -1|73.0| 74|  7| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|55.0|  75|  50|[0.0,-27.0,74.0,0...|      3951|\n",
      "|5290|  0|-26.0|-26|  1|73.0| 74|  0| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|55.0| 100|  25|[0.0,-26.0,74.0,0...|      5667|\n",
      "|5294|  1|-59.0|-59| 12|94.0| 97|  2| 1.0|  0|  -1| 1.0|   1|   0| 2.0|   3|   0|65.0|  80|  40|[1.0,-59.0,97.0,0...|      4544|\n",
      "|5361|  1|-63.0|-65| -5|88.0| 90|  1| 8.0|  6|  -2| 0.0|   1|   0| 2.0|   2|   0|76.0|  44| -22|[1.0,-65.0,90.0,6...|      3936|\n",
      "|5295|  0|-50.0|-54| -7|74.0| 77|  4| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|83.0|  90| -10|[0.0,-54.0,77.0,0...|      4891|\n",
      "|5361|  1|-63.0|-69| -4|88.0| 91|  1| 8.0|  6|   0| 0.0|   0|  -1| 2.0|   2|   0|76.0|  80|  36|[1.0,-69.0,91.0,6...|       457|\n",
      "|5361|  1|-63.0|-65|  4|88.0| 90| -1| 8.0|  7|   1| 0.0|   1|   1| 2.0|   2|   0|76.0|  66| -14|[1.0,-65.0,90.0,7...|      8336|\n",
      "|5295|  0|-49.0|-53| -6|76.0| 85| 12| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|83.0|  90| -10|[0.0,-53.0,85.0,0...|      5239|\n",
      "|4989|  1|-46.0|-47| -1|94.0| 94| -1| 0.0|  0|   0| 1.0|   1|   0| 2.0|   2|   0|94.0|  90| -10|[1.0,-47.0,94.0,0...|       522|\n",
      "|5191|  1|-30.0|-28| -1|61.0| 90|  3|26.0|  2|  -2| 2.0|   3|  -2| 1.0|   1|   0|81.0|  80|  -8|[1.0,-28.0,90.0,2...|      5365|\n",
      "|5295|  0|-49.0|-52| -5|76.0| 85| 12| 0.0|  0|   0| 0.0|   0|   0| 1.0|   1|   0|83.0|  90| -10|[0.0,-52.0,85.0,0...|      5239|\n",
      "|5361|  1|-62.0|-59| 10|88.0| 89| -2| 8.0|  8|   2| 0.0|   1|   1| 2.0|   2|   0|76.0|  66| -14|[1.0,-59.0,89.0,8...|       753|\n",
      "|5191|  1|-30.0|-32| -4|61.0| 92|  2|26.0|  1|  -1| 2.0|   2|  -1| 1.0|   1|   0|81.0|  90|  10|[1.0,-32.0,92.0,1...|      6506|\n",
      "|4989|  1|-46.0|-46|  1|94.0| 95|  1| 0.0|  0|   0| 1.0|   1|   0| 2.0|   2|   0|94.0| 100|  10|[1.0,-46.0,95.0,0...|      7416|\n",
      "+----+---+-----+---+---+----+---+---+----+---+----+----+----+----+----+----+----+----+----+----+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 4.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 222 ms (k=9999)\n",
    "\n",
    "'''\n",
    "Transform all samples to learn their groups. Each group \n",
    "corresponds to a network 'profile' (in a form of their indexes)\n",
    "'''\n",
    "df_clustered = model.transform(df_features)\n",
    "df_clustered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+\n",
      "| _c0|_c18|prediction|\n",
      "+----+----+----------+\n",
      "|5358|  88|      2559|\n",
      "|5294|  90|       663|\n",
      "|5295| 100|      7146|\n",
      "|5290|  50|      4408|\n",
      "|5290|  25|      6041|\n",
      "|5290|  75|      5551|\n",
      "|5290|  75|      3951|\n",
      "|5290| 100|      5667|\n",
      "|5294|  80|      4544|\n",
      "|5361|  44|      3936|\n",
      "|5295|  90|      4891|\n",
      "|5361|  80|       457|\n",
      "|5361|  66|      8336|\n",
      "|5295|  90|      5239|\n",
      "|4989|  90|       522|\n",
      "|5191|  80|      5365|\n",
      "|5295|  90|      5239|\n",
      "|5361|  66|       753|\n",
      "|5191|  90|      6506|\n",
      "|4989| 100|      7416|\n",
      "+----+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "From K-Mean result, extract '_c0', '_c18' and 'prediction' columns. \n",
    "\n",
    "Those columns actually have the following semantic meaning and mappings:\n",
    "     _c0       : network enpoint id             <==> 'user'   part for ALS recommender\n",
    "     prediction: network profile number         <==> 'item'   part for ALS recommender\n",
    "     _c18      : effective network throughput   <==> 'rating' part for ALS recommender\n",
    "'''\n",
    "\n",
    "df_merged = df_clustered.select('_c0', '_c18', 'prediction')\n",
    "df_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "| sta|profile| phyr|\n",
      "+----+-------+-----+\n",
      "|4718|   3755| 72.0|\n",
      "|4700|   2595| 27.0|\n",
      "|4566|   6003| 67.0|\n",
      "|4248|   8372| 59.0|\n",
      "|4597|   6165| 44.0|\n",
      "|4129|   6495|100.0|\n",
      "|4356|   1905|100.0|\n",
      "|4356|   7291| 81.0|\n",
      "|5194|   4858| 80.0|\n",
      "|4262|   7627| 66.0|\n",
      "|4180|   3787| 81.0|\n",
      "|4409|   8502| 29.5|\n",
      "|4734|   6093| 90.0|\n",
      "|4028|   1106|100.0|\n",
      "|4359|   3731| 19.6|\n",
      "|4156|   1314| 30.0|\n",
      "|4516|   8474|100.0|\n",
      "|4160|   5755| 47.0|\n",
      "|4144|   3060| 90.0|\n",
      "|4387|    320|100.0|\n",
      "+----+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 1min (k=99,999)\n",
    "\n",
    "'''\n",
    "Aggregate (_c0, _c18, prediction) rows to UNIQUE rows (= endpoint, profile, rating).\n",
    "The rows will become the training/testing data of our ALS model below.\n",
    "'''\n",
    "df_merged.createOrReplaceTempView(\"merged\")\n",
    "df_ratings = spark_sql.sql('''\n",
    "    SELECT _c0 as sta, prediction as profile, avg(_c18) as phyr\n",
    "    FROM merged\n",
    "    GROUP BY _c0, prediction\n",
    "    ''')\n",
    "# df_ratings.explain()\n",
    "df_ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 17min 57s (2m rows)\n",
    "\n",
    "# split df_ratings to training and test sets\n",
    "(training, test) = df_ratings.randomSplit([0.95, 0.05])\n",
    "\n",
    "'''\n",
    "Another data cleanse is needed here, because SPARK ML default 'cold start'\n",
    "strategy is 'nan' which is problematic at cross verification stage. This cleanse \n",
    "is to make sure all endpoints and profiles in test set are also in training set, \n",
    "else ML evaluator returns 'NaN' when predict on unknown STA or profile\n",
    "'''\n",
    "coldStartStrategy=\"drop\"   # set to drop so no cleanse is ok for SPARK ML cold start\n",
    "if coldStartStrategy != 'drop':\n",
    "    stas = {}\n",
    "    profiles = {}\n",
    "\n",
    "    # collect all STAs and profiles that are in training set.\n",
    "    for x in training.rdd.collect():\n",
    "        if x.sta not in stas:\n",
    "            stas[x.sta] = True\n",
    "        if x.profile not in profiles:\n",
    "            profiles[x.profile] = True\n",
    "    print training.count()\n",
    "\n",
    "    # collect STAs and profiles that are in test data set but \n",
    "    # not in training data set to a 'patch data set'\n",
    "    patch = []\n",
    "    for x in test.rdd.collect():\n",
    "        if x.sta not in stas or x.profile not in profiles:\n",
    "            patch.append(x)\n",
    "\n",
    "    # add the patch data set to training data set\n",
    "    print len(patch)\n",
    "    training = training.union(sc.parallelize(patch).toDF())\n",
    "    print training.count()\n",
    "\n",
    "training.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 8.72 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 27h 1min 21s (2M rows !!!)\n",
    "\n",
    "'''\n",
    "Use Spark ML CrossValidator for \"Model selection\" and build an ALS model. \n",
    "\n",
    "* This is deprecated, as CrossValidator iterates ALL combinations \n",
    "* of hyper-parameters so it takes lots of time compared with using HIPA.\n",
    "'''\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Skipped. Use my own 'HIPA' method due to CrossValidator performance issue. \n",
    "# Also, RegressionEvaluator.isBiggerBetter() is supported only in SPARK 2.2+\n",
    "if False:\n",
    "    als = ALS(maxIter=10, \n",
    "              userCol=\"sta\", itemCol=\"profile\", ratingCol=\"phyr\", \n",
    "              implicitPrefs=False, coldStartStrategy = coldStartStrategy)\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(als.rank, range(7, 10, 1))\\\n",
    "        .addGrid(als.regParam, np.arange(0.1, 2.0, 0.5))\\\n",
    "        .build()\n",
    "\n",
    "    evaluator_mae  = RegressionEvaluator(metricName=\"mae\",  labelCol=\"phyr\", predictionCol=\"prediction\")\n",
    "    evaluator_rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"phyr\", predictionCol=\"prediction\")\n",
    "\n",
    "    # !!! Somehow CrossValidator always returns the first set on the parameter grid.\n",
    "    # !!! Checked RegressionEvaluator.isLargerBetter(). It is False. Why does it fail...???\n",
    "    # *** Checked SPARK versions and found RegressionEvaluator.isLargerBetter() is only in v2.2.0+.\n",
    "    crossValidator = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator_mae,  # use mae for now\n",
    "        numFolds=5)\n",
    "\n",
    "    # run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossValidator.fit(training)\n",
    "    print \"RegressionEvaluator(mae) =\", evaluator_mae.isLargerBetter()\n",
    "\n",
    "    # test the model with the best parameters\n",
    "    predictions = cvModel.transform(test)\n",
    "\n",
    "    # Evaluate the model by computing the MAE on the test data\n",
    "    print \"Mean-average error = %.4f\"     % evaluator_mae .evaluate(predictions)\n",
    "    print \"Root-mean-square error = %.4f\" % evaluator_rmse.evaluate(predictions)\n",
    "\n",
    "    # know best rank/reg\n",
    "    print \"best rank = %d\" % cvModel.bestModel.rank\n",
    "    print \"best reg = %.1f\" % cvModel.bestModel._java_obj.parent().getRegParam()\n",
    "\n",
    "    plot_chart(test, predictions, 'spark_kmean_9999_als_2m_CrossValidator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Plot only the first nc samples, because squeezing all samples on a \n",
    "chart makes it look messy so not a good data visualization approach.\n",
    "'''\n",
    "def plot_chart(predictions, title, nc=0, sorty=True):\n",
    "    if nc == 0: \n",
    "        nc = predictions.count()\n",
    "\n",
    "    truths   = predictions.select(\"phyr\").head(nc)\n",
    "    predicts = predictions.select(\"prediction\").head(nc)\n",
    "    \n",
    "    # sort the two lists \"synchronously\" based one to avoid a fluctuating look\n",
    "    if sorty:\n",
    "        truths, predicts = zip(*sorted(zip(truths, predicts)))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 8)\n",
    "    \n",
    "    plt.scatter(np.arange(0,len(predicts)), predicts, c='r', s=2, label='prediction')\n",
    "    plt.scatter(np.arange(0,len(truths)),   truths,   c='b', s=2, label='ground truth')\n",
    "    plt.axis('tight')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.savefig(\"/data/tmp/%s.png\" % title)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 520 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Only in SPARK 2.2.0+ can CrossValidator support RegressionEvaluator.isLargerBetter()\n",
    "Even CrossValidator works, it took almost days to iterate thru entire parameter grid.\n",
    "My 'HIPA' method seems to work much faster and equally good on accuracy.\n",
    "'''\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import sys\n",
    "\n",
    "nhit = 0\n",
    "maes = {}\n",
    "rmses = {}\n",
    "best_model = ''\n",
    "min_error = sys.float_info.max\n",
    "\n",
    "def hipa_key(rank, reg, maxIter=1):\n",
    "    return '%2d, %.2f, %2d' % (rank, reg, maxIter)\n",
    "\n",
    "def hipa_fit(rank, reg, maxIter=1):\n",
    "    als = ALS(maxIter=maxIter, rank=rank, regParam=reg, \n",
    "            userCol=\"sta\", itemCol=\"profile\", ratingCol=\"phyr\",\n",
    "            implicitPrefs=False, coldStartStrategy=coldStartStrategy)\n",
    "    model = als.fit(training)\n",
    "    return model\n",
    "\n",
    "def hipa_err(mae, rmse, basedOn='rmse'):\n",
    "    return mae if basedOn == 'mae' else rmse;\n",
    "\n",
    "def hipa_estimate(rank, reg, maxIter=1, basedOn='rmse'):\n",
    "    # check cached errs to avoid re-calc \n",
    "    key = hipa_key(rank, reg, maxIter);\n",
    "    if key in maes:\n",
    "        return key, hipa_err(maes[key], rmses[key], basedOn), True\n",
    "\n",
    "    # fit and test model with this parameter tuple\n",
    "    model = hipa_fit(rank, reg, maxIter)\n",
    "    predictions = model.transform(test)\n",
    "    \n",
    "    # eval errs\n",
    "    mae  = RegressionEvaluator(metricName=\"mae\",  labelCol=\"phyr\", predictionCol=\"prediction\").evaluate(predictions)\n",
    "    rmse = RegressionEvaluator(metricName=\"rmse\", labelCol=\"phyr\", predictionCol=\"prediction\").evaluate(predictions)\n",
    "    \n",
    "    # cache for later sort to top 10 sets of parameters\n",
    "    maes [key] = mae\n",
    "    rmses[key] = rmse\n",
    "\n",
    "    # save best model so that no need to re-fit after cross validations\n",
    "    global min_error\n",
    "    global best_model\n",
    "    if (basedOn == 'mae'  and mae  < min_error) \\\n",
    "    or (basedOn == 'rmse' and rmse < min_error):\n",
    "        min_error = mae if basedOn == 'mae' else rmse;\n",
    "        best_model = model\n",
    "    \n",
    "    return key, hipa_err(mae, rmse, basedOn), False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmsiz = [41, 20, 6]\n",
      "pidxs[0]=41=[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60] \n",
      "pidxs[1]=3=[0.10000000000000001, 1.1000000000000001, 2.0] \n",
      "pidxs[2]=3=[5, 8, 10] \n",
      "           [0, 0, 0] rmse(20, 0.10,  5) = 6.33535  *\n",
      "           [1, 0, 0] rmse(21, 0.10,  5) = 5.50039  *\n",
      "           [2, 0, 0] rmse(22, 0.10,  5) = 5.27206  *\n",
      "           [3, 0, 0] rmse(23, 0.10,  5) = 5.26172  *\n",
      "           [4, 0, 0] rmse(24, 0.10,  5) = 5.35081  \n",
      "           [5, 0, 0] rmse(25, 0.10,  5) = 5.52540  \n",
      "           [6, 0, 0] rmse(26, 0.10,  5) = 5.75163  \n",
      "           [7, 0, 0] rmse(27, 0.10,  5) = 5.46309  \n",
      "           [8, 0, 0] rmse(28, 0.10,  5) = 6.00514  \n",
      "           [9, 0, 0] rmse(29, 0.10,  5) = 6.56530  \n",
      "          [10, 0, 0] rmse(30, 0.10,  5) = 5.94483  \n",
      "          [11, 0, 0] rmse(31, 0.10,  5) = 6.48453  \n",
      "          [12, 0, 0] rmse(32, 0.10,  5) = 6.12759  \n",
      "          [13, 0, 0] rmse(33, 0.10,  5) = 6.38408  \n",
      "          [14, 0, 0] rmse(34, 0.10,  5) = 5.79837  \n",
      "          [15, 0, 0] rmse(35, 0.10,  5) = 6.39561  \n",
      "          [16, 0, 0] rmse(36, 0.10,  5) = 7.16719  \n",
      "          [17, 0, 0] rmse(37, 0.10,  5) = 6.73831  \n",
      "          [18, 0, 0] rmse(38, 0.10,  5) = 7.18607  \n",
      "          [19, 0, 0] rmse(39, 0.10,  5) = 6.75647  \n",
      "          [20, 0, 0] rmse(40, 0.10,  5) = 6.54859  \n",
      "          [21, 0, 0] rmse(41, 0.10,  5) = 6.17601  \n",
      "          [22, 0, 0] rmse(42, 0.10,  5) = 6.83491  \n",
      "          [23, 0, 0] rmse(43, 0.10,  5) = 6.10769  \n",
      "          [24, 0, 0] rmse(44, 0.10,  5) = 6.36444  \n",
      "          [25, 0, 0] rmse(45, 0.10,  5) = 7.53110  \n",
      "          [26, 0, 0] rmse(46, 0.10,  5) = 7.44675  \n",
      "          [27, 0, 0] rmse(47, 0.10,  5) = 6.83783  \n",
      "          [28, 0, 0] rmse(48, 0.10,  5) = 6.43860  \n",
      "          [29, 0, 0] rmse(49, 0.10,  5) = 7.74038  \n",
      "          [30, 0, 0] rmse(50, 0.10,  5) = 7.11118  \n",
      "          [31, 0, 0] rmse(51, 0.10,  5) = 7.12141  \n",
      "          [32, 0, 0] rmse(52, 0.10,  5) = 6.69673  \n",
      "          [33, 0, 0] rmse(53, 0.10,  5) = 7.75447  \n",
      "          [34, 0, 0] rmse(54, 0.10,  5) = 7.71513  \n",
      "          [35, 0, 0] rmse(55, 0.10,  5) = 7.44990  \n",
      "          [36, 0, 0] rmse(56, 0.10,  5) = 7.46816  \n",
      "          [37, 0, 0] rmse(57, 0.10,  5) = 8.13296  \n",
      "          [38, 0, 0] rmse(58, 0.10,  5) = 7.89457  \n",
      "          [39, 0, 0] rmse(59, 0.10,  5) = 8.20119  \n",
      "          [40, 0, 0] rmse(60, 0.10,  5) = 6.76390  \n",
      "          [0, 10, 0] rmse(20, 1.10,  5) = 4.52982  *\n",
      "          [1, 10, 0] rmse(21, 1.10,  5) = 3.94899  *\n",
      "          [2, 10, 0] rmse(22, 1.10,  5) = 3.88068  *\n",
      "          [3, 10, 0] rmse(23, 1.10,  5) = 3.96158  \n",
      "          [4, 10, 0] rmse(24, 1.10,  5) = 3.70420 - *\n",
      "          [5, 10, 0] rmse(25, 1.10,  5) = 3.58518  *\n",
      "          [6, 10, 0] rmse(26, 1.10,  5) = 3.70837  \n",
      "          [7, 10, 0] rmse(27, 1.10,  5) = 4.07039  \n",
      "          [8, 10, 0] rmse(28, 1.10,  5) = 4.12477  \n",
      "          [9, 10, 0] rmse(29, 1.10,  5) = 4.48981  \n",
      "         [10, 10, 0] rmse(30, 1.10,  5) = 3.77758 - \n",
      "         [11, 10, 0] rmse(31, 1.10,  5) = 4.14309  \n",
      "         [12, 10, 0] rmse(32, 1.10,  5) = 4.19352  \n",
      "         [13, 10, 0] rmse(33, 1.10,  5) = 3.93860  \n",
      "         [14, 10, 0] rmse(34, 1.10,  5) = 3.84712  \n",
      "         [15, 10, 0] rmse(35, 1.10,  5) = 4.00599  \n",
      "         [16, 10, 0] rmse(36, 1.10,  5) = 4.11974 - \n",
      "         [17, 10, 0] rmse(37, 1.10,  5) = 4.22779  \n",
      "         [18, 10, 0] rmse(38, 1.10,  5) = 4.50176  \n",
      "         [19, 10, 0] rmse(39, 1.10,  5) = 4.25421  \n",
      "         [20, 10, 0] rmse(40, 1.10,  5) = 4.04364  \n",
      "         [21, 10, 0] rmse(41, 1.10,  5) = 3.77438  \n",
      "         [22, 10, 0] rmse(42, 1.10,  5) = 4.29163 - \n",
      "         [23, 10, 0] rmse(43, 1.10,  5) = 3.94359  \n",
      "         [24, 10, 0] rmse(44, 1.10,  5) = 4.08914  \n",
      "         [25, 10, 0] rmse(45, 1.10,  5) = 4.52888  \n",
      "         [26, 10, 0] rmse(46, 1.10,  5) = 4.44450  \n",
      "         [27, 10, 0] rmse(47, 1.10,  5) = 3.93644  \n",
      "         [28, 10, 0] rmse(48, 1.10,  5) = 4.28621 - \n",
      "         [29, 10, 0] rmse(49, 1.10,  5) = 4.58754  \n",
      "         [30, 10, 0] rmse(50, 1.10,  5) = 4.30775  \n",
      "         [31, 10, 0] rmse(51, 1.10,  5) = 4.10287  \n",
      "         [32, 10, 0] rmse(52, 1.10,  5) = 4.20179  \n",
      "         [33, 10, 0] rmse(53, 1.10,  5) = 4.46705  \n",
      "         [34, 10, 0] rmse(54, 1.10,  5) = 4.40482 - \n",
      "         [35, 10, 0] rmse(55, 1.10,  5) = 4.17355  \n",
      "         [36, 10, 0] rmse(56, 1.10,  5) = 4.35094  \n",
      "         [37, 10, 0] rmse(57, 1.10,  5) = 4.67604  \n",
      "         [38, 10, 0] rmse(58, 1.10,  5) = 4.51305  \n",
      "         [39, 10, 0] rmse(59, 1.10,  5) = 4.53795  \n",
      "         [40, 10, 0] rmse(60, 1.10,  5) = 4.23577 - \n",
      "          [0, 19, 0] rmse(20, 2.00,  5) = 5.66770  \n",
      "          [1, 19, 0] rmse(21, 2.00,  5) = 5.33384  \n",
      "          [2, 19, 0] rmse(22, 2.00,  5) = 5.48791  \n",
      "          [3, 19, 0] rmse(23, 2.00,  5) = 5.40222  \n",
      "          [4, 19, 0] rmse(24, 2.00,  5) = 5.27876  \n",
      "          [5, 19, 0] rmse(25, 2.00,  5) = 5.44576  \n",
      "          [6, 19, 0] rmse(26, 2.00,  5) = 5.41067  \n",
      "          [7, 19, 0] rmse(27, 2.00,  5) = 5.67242  \n",
      "          [8, 19, 0] rmse(28, 2.00,  5) = 5.56650  \n",
      "          [9, 19, 0] rmse(29, 2.00,  5) = 5.88529  \n",
      "         [10, 19, 0] rmse(30, 2.00,  5) = 5.53604  \n",
      "         [11, 19, 0] rmse(31, 2.00,  5) = 5.50334  \n",
      "         [12, 19, 0] rmse(32, 2.00,  5) = 5.74071  \n",
      "         [13, 19, 0] rmse(33, 2.00,  5) = 5.60777  \n",
      "         [14, 19, 0] rmse(34, 2.00,  5) = 5.64370  \n",
      "         [15, 19, 0] rmse(35, 2.00,  5) = 5.64443  \n",
      "         [16, 19, 0] rmse(36, 2.00,  5) = 5.49731  \n",
      "         [17, 19, 0] rmse(37, 2.00,  5) = 5.67016  \n",
      "         [18, 19, 0] rmse(38, 2.00,  5) = 5.96812  \n",
      "         [19, 19, 0] rmse(39, 2.00,  5) = 5.92477  \n",
      "         [20, 19, 0] rmse(40, 2.00,  5) = 5.63615  \n",
      "         [21, 19, 0] rmse(41, 2.00,  5) = 5.59115  \n",
      "         [22, 19, 0] rmse(42, 2.00,  5) = 5.93834  \n",
      "         [23, 19, 0] rmse(43, 2.00,  5) = 5.62681  \n",
      "         [24, 19, 0] rmse(44, 2.00,  5) = 5.72854  \n",
      "         [25, 19, 0] rmse(45, 2.00,  5) = 5.97422  \n",
      "         [26, 19, 0] rmse(46, 2.00,  5) = 5.84412  \n",
      "         [27, 19, 0] rmse(47, 2.00,  5) = 5.60074  \n",
      "         [28, 19, 0] rmse(48, 2.00,  5) = 5.89303  \n",
      "         [29, 19, 0] rmse(49, 2.00,  5) = 5.93050  \n",
      "         [30, 19, 0] rmse(50, 2.00,  5) = 5.73518  \n",
      "         [31, 19, 0] rmse(51, 2.00,  5) = 5.62500  \n",
      "         [32, 19, 0] rmse(52, 2.00,  5) = 5.76377  \n",
      "         [33, 19, 0] rmse(53, 2.00,  5) = 5.92371  \n",
      "         [34, 19, 0] rmse(54, 2.00,  5) = 5.72763  \n",
      "         [35, 19, 0] rmse(55, 2.00,  5) = 5.91870  \n",
      "         [36, 19, 0] rmse(56, 2.00,  5) = 5.93013  \n",
      "         [37, 19, 0] rmse(57, 2.00,  5) = 6.14711  \n",
      "         [38, 19, 0] rmse(58, 2.00,  5) = 5.96144  \n",
      "         [39, 19, 0] rmse(59, 2.00,  5) = 5.99430  \n",
      "         [40, 19, 0] rmse(60, 2.00,  5) = 5.82486  \n",
      "           [0, 0, 3] rmse(20, 0.10,  8) = 5.09470 - \n",
      "           [1, 0, 3] rmse(21, 0.10,  8) = 4.53507 - \n",
      "           [2, 0, 3] rmse(22, 0.10,  8) = 4.41408 - \n",
      "           [3, 0, 3] rmse(23, 0.10,  8) = 4.33503 - \n",
      "           [4, 0, 3] rmse(24, 0.10,  8) = 4.56361 - \n",
      "           [5, 0, 3] rmse(25, 0.10,  8) = 4.56031 - \n",
      "           [6, 0, 3] rmse(26, 0.10,  8) = 4.68116 - \n",
      "           [7, 0, 3] rmse(27, 0.10,  8) = 4.45438 - \n",
      "           [8, 0, 3] rmse(28, 0.10,  8) = 4.80143 - \n",
      "           [9, 0, 3] rmse(29, 0.10,  8) = 5.52383 - \n",
      "          [10, 0, 3] rmse(30, 0.10,  8) = 5.02782 - \n",
      "          [11, 0, 3] rmse(31, 0.10,  8) = 5.23349 - \n",
      "          [12, 0, 3] rmse(32, 0.10,  8) = 5.04525 - \n",
      "          [13, 0, 3] rmse(33, 0.10,  8) = 5.20446 - \n",
      "          [14, 0, 3] rmse(34, 0.10,  8) = 4.89246 - \n",
      "          [15, 0, 3] rmse(35, 0.10,  8) = 5.18394 - \n",
      "          [16, 0, 3] rmse(36, 0.10,  8) = 5.79061 - \n",
      "          [17, 0, 3] rmse(37, 0.10,  8) = 5.42424 - \n",
      "          [18, 0, 3] rmse(38, 0.10,  8) = 5.57274 - \n",
      "          [19, 0, 3] rmse(39, 0.10,  8) = 5.38233 - \n",
      "          [20, 0, 3] rmse(40, 0.10,  8) = 5.36356 - \n",
      "          [21, 0, 3] rmse(41, 0.10,  8) = 5.02286 - \n",
      "          [22, 0, 3] rmse(42, 0.10,  8) = 5.69502 - \n",
      "          [23, 0, 3] rmse(43, 0.10,  8) = 5.19969 - \n",
      "          [24, 0, 3] rmse(44, 0.10,  8) = 5.14045 - \n",
      "          [25, 0, 3] rmse(45, 0.10,  8) = 6.14590 - \n",
      "          [26, 0, 3] rmse(46, 0.10,  8) = 5.91480 - \n",
      "          [27, 0, 3] rmse(47, 0.10,  8) = 5.49337 - \n",
      "          [28, 0, 3] rmse(48, 0.10,  8) = 5.32404 - \n",
      "          [29, 0, 3] rmse(49, 0.10,  8) = 6.15434 - \n",
      "          [30, 0, 3] rmse(50, 0.10,  8) = 5.91849 - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [31, 0, 3] rmse(51, 0.10,  8) = 5.79567  \n",
      "          [32, 0, 3] rmse(52, 0.10,  8) = 5.62208  \n",
      "          [33, 0, 3] rmse(53, 0.10,  8) = 6.37406  \n",
      "          [34, 0, 3] rmse(54, 0.10,  8) = 6.41717  \n",
      "          [35, 0, 3] rmse(55, 0.10,  8) = 6.04565  \n",
      "          [36, 0, 3] rmse(56, 0.10,  8) = 6.14883  \n",
      "          [37, 0, 3] rmse(57, 0.10,  8) = 6.53559  \n",
      "          [38, 0, 3] rmse(58, 0.10,  8) = 6.33967  \n",
      "          [39, 0, 3] rmse(59, 0.10,  8) = 6.53642  \n",
      "          [40, 0, 3] rmse(60, 0.10,  8) = 5.67279  \n",
      "          [0, 10, 3] rmse(20, 1.10,  8) = 3.27105 - *\n",
      "          [1, 10, 3] rmse(21, 1.10,  8) = 2.93696 - *\n",
      "          [2, 10, 3] rmse(22, 1.10,  8) = 2.94221 - \n",
      "          [3, 10, 3] rmse(23, 1.10,  8) = 2.99566 - \n",
      "          [4, 10, 3] rmse(24, 1.10,  8) = 2.82346 - *\n",
      "          [5, 10, 3] rmse(25, 1.10,  8) = 2.78330 - *\n",
      "          [6, 10, 3] rmse(26, 1.10,  8) = 2.86711 - \n",
      "          [7, 10, 3] rmse(27, 1.10,  8) = 3.05688 - \n",
      "          [8, 10, 3] rmse(28, 1.10,  8) = 3.07389 - \n",
      "          [9, 10, 3] rmse(29, 1.10,  8) = 3.41835 - \n",
      "         [10, 10, 3] rmse(30, 1.10,  8) = 2.88458 - \n",
      "         [11, 10, 3] rmse(31, 1.10,  8) = 3.10519 - \n",
      "         [12, 10, 3] rmse(32, 1.10,  8) = 3.17264 - \n",
      "         [13, 10, 3] rmse(33, 1.10,  8) = 3.02700 - \n",
      "         [14, 10, 3] rmse(34, 1.10,  8) = 2.97381 - \n",
      "         [15, 10, 3] rmse(35, 1.10,  8) = 3.02437 - \n",
      "         [16, 10, 3] rmse(36, 1.10,  8) = 3.12531 - \n",
      "         [17, 10, 3] rmse(37, 1.10,  8) = 3.14827 - \n",
      "         [18, 10, 3] rmse(38, 1.10,  8) = 3.33471 - \n",
      "         [19, 10, 3] rmse(39, 1.10,  8) = 3.21117 - \n",
      "         [20, 10, 3] rmse(40, 1.10,  8) = 3.07314 - \n",
      "         [21, 10, 3] rmse(41, 1.10,  8) = 2.96482 - \n",
      "         [22, 10, 3] rmse(42, 1.10,  8) = 3.21064 - \n",
      "         [23, 10, 3] rmse(43, 1.10,  8) = 3.03577 - \n",
      "         [24, 10, 3] rmse(44, 1.10,  8) = 3.11745 - \n",
      "         [25, 10, 3] rmse(45, 1.10,  8) = 3.30016 - \n",
      "         [26, 10, 3] rmse(46, 1.10,  8) = 3.25189 - \n",
      "         [27, 10, 3] rmse(47, 1.10,  8) = 3.05520 - \n",
      "         [28, 10, 3] rmse(48, 1.10,  8) = 3.24960 - \n",
      "         [29, 10, 3] rmse(49, 1.10,  8) = 3.31529 - \n",
      "         [30, 10, 3] rmse(50, 1.10,  8) = 3.19950 - \n",
      "         [31, 10, 3] rmse(51, 1.10,  8) = 3.11889  \n",
      "         [32, 10, 3] rmse(52, 1.10,  8) = 3.18280  \n",
      "         [33, 10, 3] rmse(53, 1.10,  8) = 3.31388  \n",
      "         [34, 10, 3] rmse(54, 1.10,  8) = 3.26926 - \n",
      "         [35, 10, 3] rmse(55, 1.10,  8) = 3.19867  \n",
      "         [36, 10, 3] rmse(56, 1.10,  8) = 3.24300  \n",
      "         [37, 10, 3] rmse(57, 1.10,  8) = 3.40982  \n",
      "         [38, 10, 3] rmse(58, 1.10,  8) = 3.37821  \n",
      "         [39, 10, 3] rmse(59, 1.10,  8) = 3.36138  \n",
      "         [40, 10, 3] rmse(60, 1.10,  8) = 3.19911 - \n",
      "          [0, 19, 3] rmse(20, 2.00,  8) = 4.20741  \n",
      "          [1, 19, 3] rmse(21, 2.00,  8) = 4.17171  \n",
      "          [2, 19, 3] rmse(22, 2.00,  8) = 4.25209  \n",
      "          [3, 19, 3] rmse(23, 2.00,  8) = 4.18007  \n",
      "          [4, 19, 3] rmse(24, 2.00,  8) = 4.12556  \n",
      "          [5, 19, 3] rmse(25, 2.00,  8) = 4.26747  \n",
      "          [6, 19, 3] rmse(26, 2.00,  8) = 4.23872  \n",
      "          [7, 19, 3] rmse(27, 2.00,  8) = 4.32318  \n",
      "          [8, 19, 3] rmse(28, 2.00,  8) = 4.25263  \n",
      "          [9, 19, 3] rmse(29, 2.00,  8) = 4.43066  \n",
      "         [10, 19, 3] rmse(30, 2.00,  8) = 4.28463 - \n",
      "         [11, 19, 3] rmse(31, 2.00,  8) = 4.24877 - \n",
      "         [12, 19, 3] rmse(32, 2.00,  8) = 4.37712 - \n",
      "         [13, 19, 3] rmse(33, 2.00,  8) = 4.30950 - \n",
      "         [14, 19, 3] rmse(34, 2.00,  8) = 4.32296 - \n",
      "         [15, 19, 3] rmse(35, 2.00,  8) = 4.31197 - \n",
      "         [16, 19, 3] rmse(36, 2.00,  8) = 4.25431 - \n",
      "         [17, 19, 3] rmse(37, 2.00,  8) = 4.31931 - \n",
      "         [18, 19, 3] rmse(38, 2.00,  8) = 4.44867 - \n",
      "         [19, 19, 3] rmse(39, 2.00,  8) = 4.43142 - \n",
      "         [20, 19, 3] rmse(40, 2.00,  8) = 4.31545 - \n",
      "         [21, 19, 3] rmse(41, 2.00,  8) = 4.33629 - \n",
      "         [22, 19, 3] rmse(42, 2.00,  8) = 4.45391 - \n",
      "         [23, 19, 3] rmse(43, 2.00,  8) = 4.32887 - \n",
      "         [24, 19, 3] rmse(44, 2.00,  8) = 4.35555 - \n",
      "         [25, 19, 3] rmse(45, 2.00,  8) = 4.43920 - \n",
      "         [26, 19, 3] rmse(46, 2.00,  8) = 4.37390 - \n",
      "         [27, 19, 3] rmse(47, 2.00,  8) = 4.32102 - \n",
      "         [28, 19, 3] rmse(48, 2.00,  8) = 4.44893 - \n",
      "         [29, 19, 3] rmse(49, 2.00,  8) = 4.40513 - \n",
      "         [30, 19, 3] rmse(50, 2.00,  8) = 4.34502 - \n",
      "         [31, 19, 3] rmse(51, 2.00,  8) = 4.32009  \n",
      "         [32, 19, 3] rmse(52, 2.00,  8) = 4.37574  \n",
      "         [33, 19, 3] rmse(53, 2.00,  8) = 4.43453  \n",
      "         [34, 19, 3] rmse(54, 2.00,  8) = 4.32949  \n",
      "         [35, 19, 3] rmse(55, 2.00,  8) = 4.47256  \n",
      "         [36, 19, 3] rmse(56, 2.00,  8) = 4.43213  \n",
      "         [37, 19, 3] rmse(57, 2.00,  8) = 4.50134  \n",
      "         [38, 19, 3] rmse(58, 2.00,  8) = 4.46842  \n",
      "         [39, 19, 3] rmse(59, 2.00,  8) = 4.46111  \n",
      "         [40, 19, 3] rmse(60, 2.00,  8) = 4.41918  \n",
      "           [0, 0, 5] rmse(20, 0.10, 10) = 4.64236 - \n",
      "           [1, 0, 5] rmse(21, 0.10, 10) = 4.09302 - \n",
      "           [2, 0, 5] rmse(22, 0.10, 10) = 4.10025 - \n",
      "           [3, 0, 5] rmse(23, 0.10, 10) = 3.98003 - \n",
      "           [4, 0, 5] rmse(24, 0.10, 10) = 4.21903 - \n",
      "           [5, 0, 5] rmse(25, 0.10, 10) = 4.14612 - \n",
      "           [6, 0, 5] rmse(26, 0.10, 10) = 4.22110 - \n",
      "           [7, 0, 5] rmse(27, 0.10, 10) = 4.08908 - \n",
      "           [8, 0, 5] rmse(28, 0.10, 10) = 4.31692 - \n",
      "           [9, 0, 5] rmse(29, 0.10, 10) = 4.98230 - \n",
      "          [10, 0, 5] rmse(30, 0.10, 10) = 4.59331 - \n",
      "          [11, 0, 5] rmse(31, 0.10, 10) = 4.62800 - \n",
      "          [12, 0, 5] rmse(32, 0.10, 10) = 4.48730 - \n",
      "          [13, 0, 5] rmse(33, 0.10, 10) = 4.74481 - \n",
      "          [14, 0, 5] rmse(34, 0.10, 10) = 4.50311 - \n",
      "          [15, 0, 5] rmse(35, 0.10, 10) = 4.65175 - \n",
      "          [16, 0, 5] rmse(36, 0.10, 10) = 5.10056 - \n",
      "          [17, 0, 5] rmse(37, 0.10, 10) = 4.83202 - \n",
      "          [18, 0, 5] rmse(38, 0.10, 10) = 4.88935 - \n",
      "          [19, 0, 5] rmse(39, 0.10, 10) = 4.83545 - \n",
      "          [20, 0, 5] rmse(40, 0.10, 10) = 4.81943 - \n",
      "          [21, 0, 5] rmse(41, 0.10, 10) = 4.50694 - \n",
      "          [22, 0, 5] rmse(42, 0.10, 10) = 5.08229 - \n",
      "          [23, 0, 5] rmse(43, 0.10, 10) = 4.72128 - \n",
      "          [24, 0, 5] rmse(44, 0.10, 10) = 4.57927 - \n",
      "          [25, 0, 5] rmse(45, 0.10, 10) = 5.49119 - \n",
      "          [26, 0, 5] rmse(46, 0.10, 10) = 5.19019 - \n",
      "          [27, 0, 5] rmse(47, 0.10, 10) = 4.95658 - \n",
      "          [28, 0, 5] rmse(48, 0.10, 10) = 4.80211 - \n",
      "          [29, 0, 5] rmse(49, 0.10, 10) = 5.39674 - \n",
      "          [30, 0, 5] rmse(50, 0.10, 10) = 5.35496 - \n",
      "          [31, 0, 5] rmse(51, 0.10, 10) = 5.15482  \n",
      "          [32, 0, 5] rmse(52, 0.10, 10) = 5.10552  \n",
      "          [33, 0, 5] rmse(53, 0.10, 10) = 5.63978  \n",
      "          [34, 0, 5] rmse(54, 0.10, 10) = 5.76150  \n",
      "          [35, 0, 5] rmse(55, 0.10, 10) = 5.34237  \n",
      "          [36, 0, 5] rmse(56, 0.10, 10) = 5.47040  \n",
      "          [37, 0, 5] rmse(57, 0.10, 10) = 5.74533  \n",
      "          [38, 0, 5] rmse(58, 0.10, 10) = 5.58728  \n",
      "          [39, 0, 5] rmse(59, 0.10, 10) = 5.69146  \n",
      "          [40, 0, 5] rmse(60, 0.10, 10) = 5.13769  \n",
      "          [0, 10, 5] rmse(20, 1.10, 10) = 2.86878 - \n",
      "          [1, 10, 5] rmse(21, 1.10, 10) = 2.66824 - *\n",
      "          [2, 10, 5] rmse(22, 1.10, 10) = 2.67179 - \n",
      "          [3, 10, 5] rmse(23, 1.10, 10) = 2.70734 - \n",
      "          [4, 10, 5] rmse(24, 1.10, 10) = 2.60404 - *\n",
      "          [5, 10, 5] rmse(25, 1.10, 10) = 2.59268 - *\n",
      "          [6, 10, 5] rmse(26, 1.10, 10) = 2.64548 - \n",
      "          [7, 10, 5] rmse(27, 1.10, 10) = 2.76416 - \n",
      "          [8, 10, 5] rmse(28, 1.10, 10) = 2.74724 - \n",
      "          [9, 10, 5] rmse(29, 1.10, 10) = 3.01456 - \n",
      "         [10, 10, 5] rmse(30, 1.10, 10) = 2.64946 - \n",
      "         [11, 10, 5] rmse(31, 1.10, 10) = 2.77142 - \n",
      "         [12, 10, 5] rmse(32, 1.10, 10) = 2.84837 - \n",
      "         [13, 10, 5] rmse(33, 1.10, 10) = 2.75458 - \n",
      "         [14, 10, 5] rmse(34, 1.10, 10) = 2.72693 - \n",
      "         [15, 10, 5] rmse(35, 1.10, 10) = 2.74950 - \n",
      "         [16, 10, 5] rmse(36, 1.10, 10) = 2.81291 - \n",
      "         [17, 10, 5] rmse(37, 1.10, 10) = 2.80119 - \n",
      "         [18, 10, 5] rmse(38, 1.10, 10) = 2.94411 - \n",
      "         [19, 10, 5] rmse(39, 1.10, 10) = 2.86453 - \n",
      "         [20, 10, 5] rmse(40, 1.10, 10) = 2.78871 - \n",
      "         [21, 10, 5] rmse(41, 1.10, 10) = 2.74013 - \n",
      "         [22, 10, 5] rmse(42, 1.10, 10) = 2.88109 - \n",
      "         [23, 10, 5] rmse(43, 1.10, 10) = 2.77852 - \n",
      "         [24, 10, 5] rmse(44, 1.10, 10) = 2.83701 - \n",
      "         [25, 10, 5] rmse(45, 1.10, 10) = 2.91976 - \n",
      "         [26, 10, 5] rmse(46, 1.10, 10) = 2.88904 - \n",
      "         [27, 10, 5] rmse(47, 1.10, 10) = 2.79416 - \n",
      "         [28, 10, 5] rmse(48, 1.10, 10) = 2.93421 - \n",
      "         [29, 10, 5] rmse(49, 1.10, 10) = 2.93264 - \n",
      "         [30, 10, 5] rmse(50, 1.10, 10) = 2.87717 - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [31, 10, 5] rmse(51, 1.10, 10) = 2.83356  \n",
      "         [32, 10, 5] rmse(52, 1.10, 10) = 2.87816  \n",
      "         [33, 10, 5] rmse(53, 1.10, 10) = 2.95309  \n",
      "         [34, 10, 5] rmse(54, 1.10, 10) = 2.91015  \n",
      "         [35, 10, 5] rmse(55, 1.10, 10) = 2.89926  \n",
      "         [36, 10, 5] rmse(56, 1.10, 10) = 2.91445  \n",
      "         [37, 10, 5] rmse(57, 1.10, 10) = 2.99405  \n",
      "         [38, 10, 5] rmse(58, 1.10, 10) = 2.99295  \n",
      "         [39, 10, 5] rmse(59, 1.10, 10) = 2.97594  \n",
      "         [40, 10, 5] rmse(60, 1.10, 10) = 2.88275  \n",
      "          [0, 19, 5] rmse(20, 2.00, 10) = 3.81660  \n",
      "          [1, 19, 5] rmse(21, 2.00, 10) = 3.83084  \n",
      "          [2, 19, 5] rmse(22, 2.00, 10) = 3.87027  \n",
      "          [3, 19, 5] rmse(23, 2.00, 10) = 3.82392  \n",
      "          [4, 19, 5] rmse(24, 2.00, 10) = 3.79279  \n",
      "          [5, 19, 5] rmse(25, 2.00, 10) = 3.89639  \n",
      "          [6, 19, 5] rmse(26, 2.00, 10) = 3.87480  \n",
      "          [7, 19, 5] rmse(27, 2.00, 10) = 3.92068  \n",
      "          [8, 19, 5] rmse(28, 2.00, 10) = 3.86510  \n",
      "          [9, 19, 5] rmse(29, 2.00, 10) = 3.97823  \n",
      "         [10, 19, 5] rmse(30, 2.00, 10) = 3.90646 - \n",
      "         [11, 19, 5] rmse(31, 2.00, 10) = 3.86923 - \n",
      "         [12, 19, 5] rmse(32, 2.00, 10) = 3.95814 - \n",
      "         [13, 19, 5] rmse(33, 2.00, 10) = 3.91153 - \n",
      "         [14, 19, 5] rmse(34, 2.00, 10) = 3.92620 - \n",
      "         [15, 19, 5] rmse(35, 2.00, 10) = 3.91914 - \n",
      "         [16, 19, 5] rmse(36, 2.00, 10) = 3.88662 - \n",
      "         [17, 19, 5] rmse(37, 2.00, 10) = 3.91051 - \n",
      "         [18, 19, 5] rmse(38, 2.00, 10) = 3.98650 - \n",
      "         [19, 19, 5] rmse(39, 2.00, 10) = 3.97656 - \n",
      "         [20, 19, 5] rmse(40, 2.00, 10) = 3.92758 - \n",
      "         [21, 19, 5] rmse(41, 2.00, 10) = 3.95114 - \n",
      "         [22, 19, 5] rmse(42, 2.00, 10) = 4.00545 - \n",
      "         [23, 19, 5] rmse(43, 2.00, 10) = 3.94154 - \n",
      "         [24, 19, 5] rmse(44, 2.00, 10) = 3.94919 - \n",
      "         [25, 19, 5] rmse(45, 2.00, 10) = 3.98452 - \n",
      "         [26, 19, 5] rmse(46, 2.00, 10) = 3.94761 - \n",
      "         [27, 19, 5] rmse(47, 2.00, 10) = 3.93368 - \n",
      "         [28, 19, 5] rmse(48, 2.00, 10) = 4.01595 - \n",
      "         [29, 19, 5] rmse(49, 2.00, 10) = 3.95662 - \n",
      "         [30, 19, 5] rmse(50, 2.00, 10) = 3.93944 - \n",
      "         [31, 19, 5] rmse(51, 2.00, 10) = 3.92919  \n",
      "         [32, 19, 5] rmse(52, 2.00, 10) = 3.96774  \n",
      "         [33, 19, 5] rmse(53, 2.00, 10) = 3.99375  \n",
      "         [34, 19, 5] rmse(54, 2.00, 10) = 3.91885  \n",
      "         [35, 19, 5] rmse(55, 2.00, 10) = 4.02161  \n",
      "         [36, 19, 5] rmse(56, 2.00, 10) = 3.98889  \n",
      "         [37, 19, 5] rmse(57, 2.00, 10) = 4.01418  \n",
      "         [38, 19, 5] rmse(58, 2.00, 10) = 4.00891  \n",
      "         [39, 19, 5] rmse(59, 2.00, 10) = 4.00382  \n",
      "         [40, 19, 5] rmse(60, 2.00, 10) = 3.98894  \n",
      "pidxs[0]=2=[25, 24] \n",
      "pidxs[1]=20=[0.10000000000000001, 0.20000000000000001, 0.30000000000000004, 0.40000000000000002, 0.5, 0.59999999999999998, 0.70000000000000007, 0.80000000000000004, 0.90000000000000002, 1.0, 1.1000000000000001, 1.2000000000000002, 1.3000000000000003, 1.4000000000000001, 1.5000000000000002, 1.6000000000000001, 1.7000000000000002, 1.8000000000000003, 1.9000000000000001, 2.0] \n",
      "pidxs[2]=2=[10, 8] \n",
      "           [5, 0, 5] rmse(25, 0.10, 10) = 4.14612 - \n",
      "           [4, 0, 5] rmse(24, 0.10, 10) = 4.21903 - \n",
      "           [5, 1, 5] rmse(25, 0.20, 10) = 3.25271 - \n",
      "           [4, 1, 5] rmse(24, 0.20, 10) = 3.34846 - \n",
      "           [5, 2, 5] rmse(25, 0.30, 10) = 2.73812 - \n",
      "           [4, 2, 5] rmse(24, 0.30, 10) = 2.81046 - \n",
      "           [5, 3, 5] rmse(25, 0.40, 10) = 2.43654 - *\n",
      "           [4, 3, 5] rmse(24, 0.40, 10) = 2.52699 - \n",
      "           [5, 4, 5] rmse(25, 0.50, 10) = 2.25603 - *\n",
      "           [4, 4, 5] rmse(24, 0.50, 10) = 2.34266 - \n",
      "           [5, 5, 5] rmse(25, 0.60, 10) = 2.18602 - *\n",
      "           [4, 5, 5] rmse(24, 0.60, 10) = 2.25561 - \n",
      "           [5, 6, 5] rmse(25, 0.70, 10) = 2.19432 - \n",
      "           [4, 6, 5] rmse(24, 0.70, 10) = 2.24932 - \n",
      "           [5, 7, 5] rmse(25, 0.80, 10) = 2.25034 - \n",
      "           [4, 7, 5] rmse(24, 0.80, 10) = 2.29615 - \n",
      "           [5, 8, 5] rmse(25, 0.90, 10) = 2.34007 - \n",
      "           [4, 8, 5] rmse(24, 0.90, 10) = 2.37750 - \n",
      "           [5, 9, 5] rmse(25, 1.00, 10) = 2.45627 - \n",
      "           [4, 9, 5] rmse(24, 1.00, 10) = 2.48247 - \n",
      "          [5, 10, 5] rmse(25, 1.10, 10) = 2.59268 - \n",
      "          [4, 10, 5] rmse(24, 1.10, 10) = 2.60404 - \n",
      "          [5, 11, 5] rmse(25, 1.20, 10) = 2.74288 - \n",
      "          [4, 11, 5] rmse(24, 1.20, 10) = 2.73660 - \n",
      "          [5, 12, 5] rmse(25, 1.30, 10) = 2.90052 - \n",
      "          [4, 12, 5] rmse(24, 1.30, 10) = 2.87540 - \n",
      "          [5, 13, 5] rmse(25, 1.40, 10) = 3.05995 - \n",
      "          [4, 13, 5] rmse(24, 1.40, 10) = 3.01653 - \n",
      "          [5, 14, 5] rmse(25, 1.50, 10) = 3.21677 - \n",
      "          [4, 14, 5] rmse(24, 1.50, 10) = 3.15691 - \n",
      "          [5, 15, 5] rmse(25, 1.60, 10) = 3.36801 - \n",
      "          [4, 15, 5] rmse(24, 1.60, 10) = 3.29427 - \n",
      "          [5, 16, 5] rmse(25, 1.70, 10) = 3.51198 - \n",
      "          [4, 16, 5] rmse(24, 1.70, 10) = 3.42713 - \n",
      "          [5, 17, 5] rmse(25, 1.80, 10) = 3.64798 - \n",
      "          [4, 17, 5] rmse(24, 1.80, 10) = 3.55466 - \n",
      "          [5, 18, 5] rmse(25, 1.90, 10) = 3.77599 - \n",
      "          [4, 18, 5] rmse(24, 1.90, 10) = 3.67654 - \n",
      "          [5, 19, 5] rmse(25, 2.00, 10) = 3.89639 - \n",
      "          [4, 19, 5] rmse(24, 2.00, 10) = 3.79279 - \n",
      "           [5, 0, 3] rmse(25, 0.10,  8) = 4.56031 - \n",
      "           [4, 0, 3] rmse(24, 0.10,  8) = 4.56361 - \n",
      "           [5, 1, 3] rmse(25, 0.20,  8) = 3.75674 - \n",
      "           [4, 1, 3] rmse(24, 0.20,  8) = 3.78111 - \n",
      "           [5, 2, 3] rmse(25, 0.30,  8) = 3.20215 - \n",
      "           [4, 2, 3] rmse(24, 0.30,  8) = 3.22554 - \n",
      "           [5, 3, 3] rmse(25, 0.40,  8) = 2.85212 - \n",
      "           [4, 3, 3] rmse(24, 0.40,  8) = 2.89065 - \n",
      "           [5, 4, 3] rmse(25, 0.50,  8) = 2.61243 - \n",
      "           [4, 4, 3] rmse(24, 0.50,  8) = 2.67408 - \n",
      "           [5, 5, 3] rmse(25, 0.60,  8) = 2.47472 - \n",
      "           [4, 5, 3] rmse(24, 0.60,  8) = 2.54319 - \n",
      "           [5, 6, 3] rmse(25, 0.70,  8) = 2.43039 - \n",
      "           [4, 6, 3] rmse(24, 0.70,  8) = 2.49807 - \n",
      "           [5, 7, 3] rmse(25, 0.80,  8) = 2.45672 - \n",
      "           [4, 7, 3] rmse(24, 0.80,  8) = 2.52237 - \n",
      "           [5, 8, 3] rmse(25, 0.90,  8) = 2.53202 - \n",
      "           [4, 8, 3] rmse(24, 0.90,  8) = 2.59407 - \n",
      "           [5, 9, 3] rmse(25, 1.00,  8) = 2.64363 - \n",
      "           [4, 9, 3] rmse(24, 1.00,  8) = 2.69781 - \n",
      "          [5, 10, 3] rmse(25, 1.10,  8) = 2.78330 - \n",
      "          [4, 10, 3] rmse(24, 1.10,  8) = 2.82346 - \n",
      "          [5, 11, 3] rmse(25, 1.20,  8) = 2.94347 - \n",
      "          [4, 11, 3] rmse(24, 1.20,  8) = 2.96365 - \n",
      "          [5, 12, 3] rmse(25, 1.30,  8) = 3.11658 - \n",
      "          [4, 12, 3] rmse(24, 1.30,  8) = 3.11269 - \n",
      "          [5, 13, 3] rmse(25, 1.40,  8) = 3.29568 - \n",
      "          [4, 13, 3] rmse(24, 1.40,  8) = 3.26599 - \n",
      "          [5, 14, 3] rmse(25, 1.50,  8) = 3.47496 - \n",
      "          [4, 14, 3] rmse(24, 1.50,  8) = 3.41984 - \n",
      "          [5, 15, 3] rmse(25, 1.60,  8) = 3.65008 - \n",
      "          [4, 15, 3] rmse(24, 1.60,  8) = 3.57144 - \n",
      "          [5, 16, 3] rmse(25, 1.70,  8) = 3.81813 - \n",
      "          [4, 16, 3] rmse(24, 1.70,  8) = 3.71880 - \n",
      "          [5, 17, 3] rmse(25, 1.80,  8) = 3.97742 - \n",
      "          [4, 17, 3] rmse(24, 1.80,  8) = 3.86065 - \n",
      "          [5, 18, 3] rmse(25, 1.90,  8) = 4.12722 - \n",
      "          [4, 18, 3] rmse(24, 1.90,  8) = 3.99632 - \n",
      "          [5, 19, 3] rmse(25, 2.00,  8) = 4.26747 - \n",
      "          [4, 19, 3] rmse(24, 2.00,  8) = 4.12556 - \n",
      "pidxs[0]=2=[25, 24] \n",
      "pidxs[1]=2=[0.59999999999999998, 0.70000000000000007] \n",
      "pidxs[2]=6=[5, 6, 7, 8, 9, 10] \n",
      "           [5, 5, 0] rmse(25, 0.60,  5) = 3.49402 - \n",
      "           [4, 5, 0] rmse(24, 0.60,  5) = 3.54810 - \n",
      "           [5, 6, 0] rmse(25, 0.70,  5) = 3.37871 - \n",
      "           [4, 6, 0] rmse(24, 0.70,  5) = 3.46490 - \n",
      "           [5, 5, 1] rmse(25, 0.60,  6) = 3.03678 - \n",
      "           [4, 5, 1] rmse(24, 0.60,  6) = 3.09283 - \n",
      "           [5, 6, 1] rmse(25, 0.70,  6) = 2.94117 - \n",
      "           [4, 6, 1] rmse(24, 0.70,  6) = 3.01534 - \n",
      "           [5, 5, 2] rmse(25, 0.60,  7) = 2.71310 - \n",
      "           [4, 5, 2] rmse(24, 0.60,  7) = 2.77541 - \n",
      "           [5, 6, 2] rmse(25, 0.70,  7) = 2.64034 - \n",
      "           [4, 6, 2] rmse(24, 0.70,  7) = 2.71089 - \n",
      "           [5, 5, 3] rmse(25, 0.60,  8) = 2.47472 - \n",
      "           [4, 5, 3] rmse(24, 0.60,  8) = 2.54319 - \n",
      "           [5, 6, 3] rmse(25, 0.70,  8) = 2.43039 - \n",
      "           [4, 6, 3] rmse(24, 0.70,  8) = 2.49807 - \n",
      "           [5, 5, 4] rmse(25, 0.60,  9) = 2.30350 - \n",
      "           [4, 5, 4] rmse(24, 0.60,  9) = 2.37438 - \n",
      "           [5, 6, 4] rmse(25, 0.70,  9) = 2.28817 - \n",
      "           [4, 6, 4] rmse(24, 0.70,  9) = 2.35020 - \n",
      "           [5, 5, 5] rmse(25, 0.60, 10) = 2.18602 - \n",
      "           [4, 5, 5] rmse(24, 0.60, 10) = 2.25561 - \n",
      "           [5, 6, 5] rmse(25, 0.70, 10) = 2.19432 - \n",
      "           [4, 6, 5] rmse(24, 0.70, 10) = 2.24932 - \n",
      "pidxs[0]=2=[24, 25] \n",
      "pidxs[1]=2=[0.59999999999999998, 0.70000000000000007] \n",
      "pidxs[2]=2=[10, 9] \n",
      "           [4, 5, 5] rmse(24, 0.60, 10) = 2.25561 - \n",
      "           [5, 5, 5] rmse(25, 0.60, 10) = 2.18602 - \n",
      "           [4, 6, 5] rmse(24, 0.70, 10) = 2.24932 - \n",
      "           [5, 6, 5] rmse(25, 0.70, 10) = 2.19432 - \n",
      "           [4, 5, 4] rmse(24, 0.60,  9) = 2.37438 - \n",
      "           [5, 5, 4] rmse(25, 0.60,  9) = 2.30350 - \n",
      "           [4, 6, 4] rmse(24, 0.70,  9) = 2.35020 - \n",
      "           [5, 6, 4] rmse(25, 0.70,  9) = 2.28817 - \n",
      "pidxs[0]=2=[25, 24] \n",
      "pidxs[1]=2=[0.59999999999999998, 0.70000000000000007] \n",
      "pidxs[2]=2=[10, 9] \n",
      "           [5, 5, 5] rmse(25, 0.60, 10) = 2.18602 - \n",
      "           [4, 5, 5] rmse(24, 0.60, 10) = 2.25561 - \n",
      "           [5, 6, 5] rmse(25, 0.70, 10) = 2.19432 - \n",
      "           [4, 6, 5] rmse(24, 0.70, 10) = 2.24932 - \n",
      "           [5, 5, 4] rmse(25, 0.60,  9) = 2.30350 - \n",
      "           [4, 5, 4] rmse(24, 0.60,  9) = 2.37438 - \n",
      "           [5, 6, 4] rmse(25, 0.70,  9) = 2.28817 - \n",
      "           [4, 6, 4] rmse(24, 0.70,  9) = 2.35020 - \n",
      "CPU times: user 3.54 s, sys: 304 ms, total: 3.84 s\n",
      "Wall time: 1h 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Depending on size of data set, this cell can take many minutes to hours. \n",
    "# It often times out the session and becomes offline, so %%time can't help\n",
    "import time\n",
    "tstart = time.time()\n",
    "\n",
    "'''\n",
    " Heuristic Interleaved Parameter Alternator (HIPA):\n",
    " \n",
    " The basics of the idea is find the best parameter set by iterating \n",
    " a large subrange of only one parameter at a time while visiting only \n",
    " few pivot values of all other parameters in (ranks, reg, alpha, lambda) \n",
    " tuples. The pivot values of a parameter are selected among those which\n",
    " have the (topmost 2 or 3) minimum RMSE errors from model training.\n",
    " \n",
    " In contrast, SPARK ML CrossValidator iterates all tuples of in a \n",
    " parameter grid, so it takes much more iterations.\n",
    " \n",
    " Can we optimize model selection use Stochastic Gradient Descent like in \n",
    " neural network or deep learning? IMO, it's hard if not can't. Values in \n",
    " parameter grids are normally discrete, so it's not relevant to apply a\n",
    " continuous function like derivative on the parameters.\n",
    "'''\n",
    "# implicit ALS needs 4 parameters: rank, alpha, lambda and iterations (ref. DAAL C++)\n",
    "# explicit ALS needs 3 parameters: rank, regulation and iterations\n",
    "ranks = np.arange(  20,  61,    1)\n",
    "regus = np.arange( 0.1, 2.1,  0.1)   # must not zero!!\n",
    "iters = np.arange(   5,  11,    1)   # max iteration\n",
    "pms   = [ranks, regus, iters]        # order is significant\n",
    "NP    = len(pms)\n",
    "\n",
    "pmsiz = [len(pms[i]) //  1 for i in range(NP)]\n",
    "print 'pmsiz = %s' % str(pmsiz)\n",
    "\n",
    "pidxs = [[0, pmsiz[i]//2, pmsiz[i]-1] for i in range(NP)]\n",
    "\n",
    "# emin[p][i] stores minimum rmse so far calculated on pms[p][i]\n",
    "# top NTOP indices will be preferred after intv[p] = 1 pass\n",
    "# !!! emin = [{}] * NP shares {} among all parameters 1!!\n",
    "emin = [{} for i in range(NP)]\n",
    "\n",
    "# this dict is to track total calcs regardless if rmses cache\n",
    "# is propagated from prior call run or not.\n",
    "calc = {}\n",
    "ncalc = 0\n",
    "\n",
    "nhit = 0\n",
    "fullp = 0\n",
    "full = [False] * NP\n",
    "min_rmse = sys.float_info.max\n",
    "nstall = 0\n",
    "NTOP = 2\n",
    "while True:\n",
    "    # collect indices of all parameters to try in this round\n",
    "    niter = 1\n",
    "    for p in range(NP):\n",
    "        # sort dict emin[p] by value. \n",
    "        ekvs = sorted(emin[p].iteritems(), key=lambda (k,v): (v,k))\n",
    "        if p == fullp:\n",
    "            if not full[p]:\n",
    "                pidxs[p] = range(pmsiz[p])\n",
    "                full[p] = True\n",
    "            else:\n",
    "                weaks = dict(ekvs[NTOP:])\n",
    "                pidxs[p] = []\n",
    "                for i in range(pmsiz[p]):\n",
    "                    if i not in weaks: pidxs[p].append(i)\n",
    "        elif emin[p]:\n",
    "            pidxs[p] = list(map(lambda (k,v): k, ekvs[0:NTOP]))\n",
    "        \n",
    "        pvals = [pms[p][i] for i in pidxs[p]]\n",
    "        print 'pidxs[%d]=%d=%s ' % (p, len(pidxs[p]), str(pvals))\n",
    "        # product of all index lengths\n",
    "        niter *= len(pidxs[p])\n",
    "\n",
    "    new_min = False\n",
    "    for it in range(niter):\n",
    "        pidx = [0] * NP\n",
    "        # update locally all parameters of this run\n",
    "        # by breaking down current 'it' value\n",
    "        tit = it\n",
    "        for p in range(NP):\n",
    "            pidx[p] = pidxs[p][tit % len(pidxs[p])]\n",
    "            tit = int(tit / len(pidxs[p]))\n",
    "\n",
    "        key, rmse, hit = hipa_estimate(ranks[pidx[0]], \n",
    "                                       regus[pidx[1]], \n",
    "                             maxIter = iters[pidx[2]])\n",
    "        if hit: nhit += 1\n",
    "        \n",
    "        # debug out of bound index\n",
    "        sidx = str(pidx)\n",
    "        print(\"%20s rmse(%s) = %.5f %s %s\" % \n",
    "              (sidx, key, rmse, ('-' if hit else ''), ('*' if min_rmse > rmse else '')))\n",
    "\n",
    "        if  min_rmse > rmse:\n",
    "            min_rmse = rmse\n",
    "            new_min = True\n",
    "\n",
    "        # track min/avg rmse of all indexes of all parameters\n",
    "        # this is to avoid revisit \"weak\" values on 2nd round\n",
    "        for p in range(NP):\n",
    "            i = pidx[p]\n",
    "            emin[p][i] = min(emin[p][i], rmse) if i in emin[p] else rmse\n",
    "\n",
    "        # this correctly counts calcs in case rmses cache is propagated.\n",
    "        # cache is propagated only in development and not in production.\n",
    "        if key not in calc: ncalc += 1\n",
    "        calc[key] = True\n",
    "        \n",
    "    fullp = (fullp + 1) % NP\n",
    "    nstall = 0 if new_min else nstall+1\n",
    "\n",
    "    # stop when no parameter improved and all parameters\n",
    "    # have been tried in the finest granularity\n",
    "    if nstall >= 1 * NP:  # full scans each param\n",
    "        break\n",
    "\n",
    "tend = time.time()\n",
    "# see next cell for print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time used     : 1:5:7\n",
      "calcs w/o hipa: 4920\n",
      "calcs w/  hipa: 453\n",
      "calc hits     : 590\n",
      "calc saving   : 90%\n",
      "\n",
      "top 10 best parameter sets:\n",
      "0: rmse 2.17300, rank  17, regu 0.70, iter 10\n",
      "1: rmse 2.18141, rank  17, regu 0.60, iter 10\n",
      "2: rmse 2.18602, rank  25, regu 0.60, iter 10\n",
      "3: rmse 2.19432, rank  25, regu 0.70, iter 10\n",
      "4: rmse 2.20672, rank  17, regu 0.80, iter 10\n",
      "5: rmse 2.24223, rank  17, regu 0.50, iter 10\n",
      "6: rmse 2.24932, rank  24, regu 0.70, iter 10\n",
      "7: rmse 2.25034, rank  25, regu 0.80, iter 10\n",
      "8: rmse 2.25561, rank  24, regu 0.60, iter 10\n",
      "9: rmse 2.25603, rank  25, regu 0.50, iter 10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Print HIPA summary and top 10 best models.\n",
    "About 80-90% trainings were saved per different parameter grids.\n",
    "RMSE is ~ 2. (ratio is 2% of the maximum value 100)\n",
    "'''\n",
    "tsec = tend - tstart\n",
    "tmin = tsec // 60\n",
    "tsec = tsec %  60\n",
    "thrs = tmin // 60\n",
    "tmin = tmin %  60\n",
    "tuse = '%d:%d:%d' % (thrs, tmin, tsec)\n",
    "\n",
    "ocalc = reduce(lambda x,y: x*y, pmsiz)  \n",
    "print 'time used     : %s' % tuse\n",
    "print 'calcs w/o hipa: %d' % ocalc \n",
    "print 'calcs w/  hipa: %d' % ncalc \n",
    "print 'calc hits     : %d' % nhit \n",
    "print 'calc saving   : %d%%' % int(100.0 * (ocalc - ncalc) / ocalc) \n",
    "print \n",
    "print 'top 10 best parameter sets:'\n",
    "\n",
    "import operator \n",
    "errors = sorted(rmses.items(), key=operator.itemgetter(1))[:10] \n",
    "for i, e in enumerate(errors): \n",
    "    rank, regu, iter = e[0].split(', ') \n",
    "    print '%d: rmse %.5f, rank %3s, regu %s, iter %s' % (i, e[1], rank, regu, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best rank, reg, iter = 17, 0.70, 10\n",
      "min_mae [17, 0.70, 10] = 1.040252\n",
      "min_rmse[17, 0.70, 10] = 2.173003\n"
     ]
    }
   ],
   "source": [
    "# show the best parameters\n",
    "mkey = errors[0][0]\n",
    "print \"best rank, reg, iter = %s\" % mkey\n",
    "print \"min_mae [%s] = %f\" % (mkey, maes [mkey])\n",
    "print \"min_rmse[%s] = %f\" % (mkey, rmses[mkey])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAAHiCAYAAADlHeELAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f227c6bd050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 960 ms, sys: 1.75 s, total: 2.71 s\n",
      "Wall time: 19.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2272d62bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "# Wall time: 19.2 s\n",
    "\n",
    "# use test data set to verify the best learned model\n",
    "predictions = best_model.transform(test)\n",
    "\n",
    "# (entertain myself) verify spark's rmse using my own calculator\n",
    "if False:\n",
    "    import math\n",
    "    rdds = predictions.rdd.map(lambda x: ((x.phyr - x.prediction)*(x.phyr - x.prediction), (x.phyr)*(x.phyr)))\n",
    "    deltas = rdds.map(lambda x: x[0]).reduce(lambda x,y: x+y)\n",
    "    imprvs = rdds.map(lambda x: x[1]).reduce(lambda x,y: x+y)\n",
    "    print 'deltas = %f' % deltas\n",
    "    print 'rdds.count = %d' % rdds.count()\n",
    "    print 'rmse = %f' % (math.sqrt(deltas/rdds.count()))\n",
    "\n",
    "# plot the predictions\n",
    "plot_chart(predictions, 'spark_kmean_%d_als_2m_CrossValidator (sorted)' % K)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| sta|     recommendations|\n",
      "+----+--------------------+\n",
      "|5300|[[8243,100.15496]...|\n",
      "|1342|[[1085,110.37859]...|\n",
      "|2122|[[1085,105.65619]...|\n",
      "|2142|[[8243,102.00564]...|\n",
      "|2366|[[4609,107.671814...|\n",
      "|1959|[[1085,101.342674...|\n",
      "|3749|[[8243,102.77281]...|\n",
      "|4519|[[1085,106.31731]...|\n",
      "|1721|[[1085,108.06614]...|\n",
      "|4161|[[8243,101.478165...|\n",
      "|3475|[[1085,106.63253]...|\n",
      "|3698|[[1085,105.278175...|\n",
      "|4158|[[169,100.6832], ...|\n",
      "|2999|[[4422,99.64819],...|\n",
      "|3179|[[7497,106.141846...|\n",
      "|1650|[[1085,106.165245...|\n",
      "|3000|[[1085,106.07077]...|\n",
      "|4391|[[1085,107.89579]...|\n",
      "|1322|[[1085,107.46966]...|\n",
      "|1352|[[8243,100.0453],...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# recommend 10 profiles for everyone\n",
    "rcmd_profiles = best_model.recommendForAllUsers(10)\n",
    "rcmd_profiles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# peek recommended profiles and rating (network throughput) of a endpoint\n",
    "profiles = rcmd_profiles.where('sta == 5300').select('recommendations.profile').collect()\n",
    "ratings  = rcmd_profiles.where('sta == 5300').select('recommendations.rating').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pair = zip(profiles[0]['profile'], ratings[0]['rating'])\n",
    "\n",
    "# from __future__ import print_function\n",
    "# map(lambda (x, y):print(x, ' - ', y), pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8243, 100.15496063232422),\n",
       " (3263, 99.41224670410156),\n",
       " (2762, 99.3109359741211),\n",
       " (5591, 99.1719970703125),\n",
       " (169, 99.09192657470703),\n",
       " (5326, 99.00418853759766),\n",
       " (7438, 98.9896011352539),\n",
       " (4974, 98.97924041748047),\n",
       " (526, 98.97040557861328),\n",
       " (6914, 98.94583129882812)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.userFactors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.itemFactors.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+----------+\n",
      "| sta|profile|phyr|prediction|\n",
      "+----+-------+----+----------+\n",
      "|3000|   1085|   0| 106.07077|\n",
      "|3000|   1504|   0| 102.09174|\n",
      "|3000|   8112|   0|101.349594|\n",
      "|3000|    402|   0| 101.34347|\n",
      "|3000|   1476|   0| 101.33288|\n",
      "|3000|   3768|   0| 101.31005|\n",
      "|3000|    117|   0| 101.29764|\n",
      "|3000|    463|   0| 101.29527|\n",
      "|3000|   5286|   0| 101.22453|\n",
      "|3000|   6285|   0| 101.09802|\n",
      "+----+-------+----+----------+\n",
      "\n",
      "CPU times: user 3.1 s, sys: 0 ns, total: 3.1 s\n",
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Let's recommend the best 10 network profiles for a specific endpoint (id = 3000)\n",
    "\n",
    "# Pro:\n",
    "#     The model recommends 10 best performance profiles that can\n",
    "#     even achieve >100% theoretic PHY rate of the endpoint.\n",
    "# Con:\n",
    "#     It took 5+ secs! Such a response time seems not good for real-time \n",
    "#     prediction in production environment. We may need massive GPU for \n",
    "#     a prediction workload of millions of network endpoints when even a small\n",
    "#     percentage (say 1%) of them may roam and need dynamic steering.\n",
    "testdata_1u = spark_sql.createDataFrame([(3000,p,0) for p in range(K)],['sta', 'profile', 'phyr'])\n",
    "predictions_1u = best_model.transform(testdata_1u)\n",
    "predictions_1u.where('isNaN(prediction) = FALSE')\\\n",
    "    .orderBy('prediction', ascending=False)\\\n",
    "    .limit(10)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
